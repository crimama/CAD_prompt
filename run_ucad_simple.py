import contextlib
import logging
import os
import sys
import argparse
from datetime import datetime

import numpy as np
import torch
import tqdm
import patchcore.common
import patchcore.metrics
import patchcore.patchcore
import patchcore.sampler
import patchcore.utils
from sklearn.metrics import roc_auc_score, average_precision_score
from scipy.ndimage import label
from bisect import bisect
import time
from metric_utils import find_optimal_threshold
import cv2
from timm.scheduler import create_scheduler
from timm.optim import create_optimizer
import yaml
import pandas as pd
import pickle
import json

LOGGER = logging.getLogger(__name__)

_DATASETS = {"mvtec": ["patchcore.datasets.mvtec", "MVTecDataset"]}


def create_experiment_directories(args):
    """ì‹¤í—˜ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
    # results/{dataset}/exp_name/seed/* êµ¬ì¡°
    if len(args.subdatasets) == 1:
        dataset_name = args.subdatasets[0]
    else:
        dataset_name = f"mvtec_{len(args.subdatasets)}classes"
    
    exp_name = args.exp_name if hasattr(args, 'exp_name') and args.exp_name else f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # ê¸°ë³¸ ì‹¤í—˜ ë””ë ‰í† ë¦¬
    experiment_dir = os.path.join(args.results_path, dataset_name, exp_name, f"seed_{args.seed}")
    
    # í•˜ìœ„ ë””ë ‰í† ë¦¬ë“¤ ìƒì„±
    subdirs = {
        'logs': os.path.join(experiment_dir, 'logs'),
        'results': os.path.join(experiment_dir, 'results'), 
        'checkpoints': os.path.join(experiment_dir, 'checkpoints'),
        'configs': os.path.join(experiment_dir, 'configs')
    }
    
    for dir_path in subdirs.values():
        os.makedirs(dir_path, exist_ok=True)
    
    return experiment_dir, subdirs


def save_experiment_config(args, config_dir):
    """ì‹¤í—˜ ì„¤ì •ì„ YAML íŒŒì¼ë¡œ ì €ì¥"""
    config_dict = vars(args).copy()
    
    # ì €ì¥í•˜ê¸° ì–´ë ¤ìš´ ê°ì²´ë“¤ ì œê±° ë˜ëŠ” ë³€í™˜
    config_dict.pop('device', None)
    config_dict.pop('device_context', None)
    
    config_path = os.path.join(config_dir, 'config.yaml')
    with open(config_path, 'w') as f:
        yaml.dump(config_dict, f, default_flow_style=False, sort_keys=True)
    
    return config_path


def setup_experiment_logger(log_dir, exp_name):
    """ì‹¤í—˜ ì „ìš© ë¡œê±° ì„¤ì •"""
    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    log_path = os.path.join(log_dir, f'{exp_name}_training.log')
    
    # ì‹¤í—˜ ë¡œê±° ìƒì„±
    exp_logger = logging.getLogger('experiment_logger')
    exp_logger.setLevel(logging.INFO)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
    for handler in exp_logger.handlers[:]:
        exp_logger.removeHandler(handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
    file_handler = logging.FileHandler(log_path, mode='w', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬ ì¶”ê°€ (ì„ íƒì )
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    
    # í¬ë§·í„° ì„¤ì •
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s', 
                                datefmt='%Y-%m-%d %H:%M:%S')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    exp_logger.addHandler(file_handler)
    exp_logger.addHandler(console_handler)
    exp_logger.propagate = False  # ìƒìœ„ ë¡œê±°ë¡œ ì „íŒŒ ë°©ì§€
    
    return exp_logger, log_path


class ResultManager:
    """ê²°ê³¼ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, results_dir):
        self.results_dir = results_dir
        self.result_csv_path = os.path.join(results_dir, 'result.csv')
        self.final_csv_path = os.path.join(results_dir, 'Final.csv')
        
        # CSV ì»¬ëŸ¼ ì •ì˜
        self.result_columns = [
            'dataset_name', 'epoch', 'split_type', 'image_auroc', 'pixel_auroc', 
            'image_ap', 'pixel_ap', 'pixel_pro', 'time_cost'
        ]
        
        self.final_columns = [
            'test_dataset', 'selected_task', 'selected_task_id', 'image_auroc', 
            'pixel_auroc', 'image_ap', 'pixel_ap', 'pixel_pro', 'time_cost'
        ]
        
        # CSV íŒŒì¼ ì´ˆê¸°í™”
        self._initialize_csv_files()
    
    def _initialize_csv_files(self):
        """CSV íŒŒì¼ ì´ˆê¸°í™”"""
        # result.csv ì´ˆê¸°í™”
        if not os.path.exists(self.result_csv_path):
            df_result = pd.DataFrame(columns=self.result_columns)
            df_result.to_csv(self.result_csv_path, index=False)
        
        # Final.csv ì´ˆê¸°í™”  
        if not os.path.exists(self.final_csv_path):
            df_final = pd.DataFrame(columns=self.final_columns)
            df_final.to_csv(self.final_csv_path, index=False)
    
    def save_epoch_result(self, dataset_name, epoch, split_type, metrics):
        """ì—í¬í¬ë³„ ê²°ê³¼ ì €ì¥"""
        new_row = {
            'dataset_name': dataset_name,
            'epoch': epoch,
            'split_type': split_type,  # 'limited_memory' or 'unlimited_memory'
            'image_auroc': metrics['auroc'],
            'pixel_auroc': metrics['full_pixel_auroc'],
            'image_ap': metrics['img_ap'],
            'pixel_ap': metrics['pixel_ap'],
            'pixel_pro': metrics['pixel_pro'],
            'time_cost': metrics['time_cost']
        }
        
        # CSV íŒŒì¼ì— ì¶”ê°€
        df = pd.DataFrame([new_row])
        df.to_csv(self.result_csv_path, mode='a', header=False, index=False)
    
    def save_final_result(self, test_dataset, selected_task, selected_task_id, metrics):
        """ìµœì¢… inference ê²°ê³¼ ì €ì¥"""
        new_row = {
            'test_dataset': test_dataset,
            'selected_task': selected_task,
            'selected_task_id': selected_task_id,
            'image_auroc': metrics['image_auroc'],
            'pixel_auroc': metrics['pixel_auroc'],
            'image_ap': metrics['image_ap'],
            'pixel_ap': metrics['pixel_ap'],
            'pixel_pro': metrics['pixel_pro'],
            'time_cost': metrics['time_cost']
        }
        
        # CSV íŒŒì¼ì— ì¶”ê°€
        df = pd.DataFrame([new_row])
        df.to_csv(self.final_csv_path, mode='a', header=False, index=False)
    
    def get_best_results_summary(self):
        """ìµœê³  ì„±ëŠ¥ ê²°ê³¼ ìš”ì•½ ë°˜í™˜"""
        if not os.path.exists(self.result_csv_path):
            return None
            
        df = pd.read_csv(self.result_csv_path)
        if df.empty:
            return None
        
        # ê° ë°ì´í„°ì…‹ë³„ ìµœê³  ì„±ëŠ¥ ì¶”ì¶œ
        best_results = []
        for dataset in df['dataset_name'].unique():
            dataset_df = df[df['dataset_name'] == dataset]
            
            # limited_memoryì™€ unlimited_memory ë³„ë¡œ ìµœê³  ì„±ëŠ¥
            for split_type in ['limited_memory', 'unlimited_memory']:
                split_df = dataset_df[dataset_df['split_type'] == split_type]
                if not split_df.empty:
                    best_idx = split_df['image_auroc'].idxmax()
                    best_row = split_df.loc[best_idx].to_dict()
                    best_results.append(best_row)
        
        return best_results


class CheckpointManager:
    """ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, checkpoint_dir):
        self.checkpoint_dir = checkpoint_dir
        self.best_prompts = {}
        self.best_memory_features = {}
        self.best_key_features = {}
        
    def save_best_checkpoint(self, dataset_name, prompt, memory_feature, key_feature, metrics):
        """ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        # í”„ë¡¬í”„íŠ¸ ì €ì¥
        prompt_path = os.path.join(self.checkpoint_dir, f'{dataset_name}_best_prompt.pkl')
        with open(prompt_path, 'wb') as f:
            pickle.dump(prompt, f)
        
        # ë©”ëª¨ë¦¬ íŠ¹ì§• ì €ì¥
        memory_path = os.path.join(self.checkpoint_dir, f'{dataset_name}_best_memory_feature.pkl')
        with open(memory_path, 'wb') as f:
            pickle.dump(memory_feature, f)
            
        # í‚¤ íŠ¹ì§• ì €ì¥
        key_path = os.path.join(self.checkpoint_dir, f'{dataset_name}_best_key_feature.pkl')
        with open(key_path, 'wb') as f:
            pickle.dump(key_feature, f)
        
        # ë©”íŠ¸ë¦­ ì •ë³´ ì €ì¥
        metrics_path = os.path.join(self.checkpoint_dir, f'{dataset_name}_best_metrics.json')
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=2)
        
        # ë©”ëª¨ë¦¬ì—ë„ ì €ì¥
        self.best_prompts[dataset_name] = prompt
        self.best_memory_features[dataset_name] = memory_feature
        self.best_key_features[dataset_name] = key_feature
        
        return {
            'prompt_path': prompt_path,
            'memory_path': memory_path, 
            'key_path': key_path,
            'metrics_path': metrics_path
        }
    
    def load_best_checkpoint(self, dataset_name):
        """ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        prompt_path = os.path.join(self.checkpoint_dir, f'{dataset_name}_best_prompt.pkl')
        memory_path = os.path.join(self.checkpoint_dir, f'{dataset_name}_best_memory_feature.pkl')
        key_path = os.path.join(self.checkpoint_dir, f'{dataset_name}_best_key_feature.pkl')
        
        try:
            with open(prompt_path, 'rb') as f:
                prompt = pickle.load(f)
            with open(memory_path, 'rb') as f:
                memory_feature = pickle.load(f)
            with open(key_path, 'rb') as f:
                key_feature = pickle.load(f)
                
            return prompt, memory_feature, key_feature
        except FileNotFoundError:
            return None, None, None


class UCADTrainer:
    """UCAD í›ˆë ¨ ë° í‰ê°€ë¥¼ ìœ„í•œ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, args):
        self.args = args
        self.device = patchcore.utils.set_torch_device(args.gpu)
        self.device_context = (
            torch.cuda.device("cuda:{}".format(self.device.index))
            if "cuda" in self.device.type.lower()
            else contextlib.suppress()
        )
        
        # ì‹¤í—˜ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
        self.experiment_dir, self.subdirs = create_experiment_directories(args)
        
        # ì‹¤í—˜ ì„¤ì • ì €ì¥
        self.config_path = save_experiment_config(args, self.subdirs['configs'])
        
        # ë¡œê±° ì„¤ì •
        exp_name = os.path.basename(os.path.dirname(self.experiment_dir))
        self.exp_logger, self.log_path = setup_experiment_logger(self.subdirs['logs'], exp_name)
        
        # ê²°ê³¼ ê´€ë¦¬ì ì´ˆê¸°í™”
        self.result_manager = ResultManager(self.subdirs['results'])
        
        # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì ì´ˆê¸°í™”
        self.checkpoint_manager = CheckpointManager(self.subdirs['checkpoints'])
        
        # ì‹¤í—˜ ì‹œì‘ ë¡œê·¸
        self.exp_logger.info("="*80)
        self.exp_logger.info(f"UCAD Experiment Started")
        self.exp_logger.info(f"Experiment Directory: {self.experiment_dir}")
        self.exp_logger.info(f"Config saved to: {self.config_path}")
        self.exp_logger.info("="*80)
        
        # ê¸°ì¡´ ë³€ìˆ˜ë“¤
        self.key_feature_list = [0] * 15
        self.memory_feature_list = [0] * 15
        self.prompt_list = [0] * 15

    def create_patchcore_model(self, input_shape, sampler):
        """PatchCore ëª¨ë¸ ìƒì„± (ViT ê¸°ë°˜, backbone ì‚¬ìš© ì•ˆí•¨)"""
        nn_method = patchcore.common.FaissNN(self.args.faiss_on_gpu, self.args.faiss_num_workers)
        
        patchcore_instance = patchcore.patchcore.PatchCore(self.device)
        patchcore_instance.load(
            backbone=None,  # backbone ì‚¬ìš© ì•ˆí•¨
            layers_to_extract_from=[],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸
            device=self.device,
            input_shape=input_shape,
            pretrain_embed_dimension=self.args.pretrain_embed_dimension,
            target_embed_dimension=self.args.target_embed_dimension,
            patchsize=self.args.patchsize,
            featuresampler=sampler,
            anomaly_scorer_num_nn=self.args.anomaly_scorer_num_nn,
            nn_method=nn_method,
        )
        
        return [patchcore_instance]

    def create_sampler(self):
        """ìƒ˜í”ŒëŸ¬ ìƒì„±"""
        if self.args.sampler_name == "identity":
            return patchcore.sampler.IdentitySampler()
        elif self.args.sampler_name == "greedy_coreset":
            return patchcore.sampler.GreedyCoresetSampler(self.args.percentage, self.device)
        elif self.args.sampler_name == "approx_greedy_coreset":
            return patchcore.sampler.ApproximateGreedyCoresetSampler(self.args.percentage, self.device)

    def create_dataloaders(self):
        """ë°ì´í„°ë¡œë” ìƒì„±"""
        dataset_info = _DATASETS[self.args.dataset_name]
        dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])

        dataloaders = []
        for subdataset in self.args.subdatasets:
            train_dataset = dataset_library.__dict__[dataset_info[1]](
                self.args.data_path,
                classname=subdataset,
                resize=self.args.resize,
                train_val_split=self.args.train_val_split,
                imagesize=self.args.imagesize,
                split=dataset_library.DatasetSplit.TRAIN,
                seed=self.args.seed,
                augment=self.args.augment,
            )

            test_dataset = dataset_library.__dict__[dataset_info[1]](
                self.args.data_path,
                classname=subdataset,
                resize=self.args.resize,
                imagesize=self.args.imagesize,
                split=dataset_library.DatasetSplit.TEST,
                seed=self.args.seed,
            )

            train_dataloader = torch.utils.data.DataLoader(
                train_dataset,
                batch_size=self.args.batch_size,
                shuffle=True,
                num_workers=self.args.num_workers,
                pin_memory=True,
            )

            test_dataloader = torch.utils.data.DataLoader(
                test_dataset,
                batch_size=self.args.batch_size,
                shuffle=False,
                num_workers=self.args.num_workers,
                pin_memory=True,
            )

            train_dataloader.name = self.args.dataset_name
            if subdataset is not None:
                train_dataloader.name += "_" + subdataset

            if self.args.train_val_split < 1:
                val_dataset = dataset_library.__dict__[dataset_info[1]](
                    self.args.data_path,
                    classname=subdataset,
                    resize=self.args.resize,
                    train_val_split=self.args.train_val_split,
                    imagesize=self.args.imagesize,
                    split=dataset_library.DatasetSplit.VAL,
                    seed=self.args.seed,
                )

                val_dataloader = torch.utils.data.DataLoader(
                    val_dataset,
                    batch_size=self.args.batch_size,
                    shuffle=False,
                    num_workers=self.args.num_workers,
                    pin_memory=True,
                )
            else:
                val_dataloader = None
                
            dataloader_dict = {
                "training": train_dataloader,
                "validation": val_dataloader,
                "testing": test_dataloader,
            }

            dataloaders.append(dataloader_dict)
        
        return dataloaders

    def train_single_dataset(self, dataloader_count, dataloaders, patchcore_list):
        """ë‹¨ì¼ ë°ì´í„°ì…‹ì— ëŒ€í•œ í›ˆë ¨"""
        dataset_name = dataloaders["training"].name
        
        # ë°ì´í„°ì…‹ ì‹œì‘ ë¡œê·¸
        self.log_dataset_start(dataset_name, dataloader_count, len(self.args.subdatasets))
        
        # PatchCore ëª¨ë¸ì€ í˜„ì¬ ì„¤ì •ì—ì„œ 1ê°œë§Œ ìˆìŒ
        patchcore = patchcore_list[0]  # ì²« ë²ˆì§¸(ìœ ì¼í•œ) ëª¨ë¸ ì‚¬ìš©
        
        # í‚¤ íŠ¹ì§• ì¶”ì¶œ
        patchcore.set_dataloadercount(dataloader_count)
        key_feature = patchcore.fit_with_limit_size(dataloaders["training"], self.args.key_size)
        self.key_feature_list[dataloader_count] = key_feature

        # í›ˆë ¨ ì„¤ì •
        args_dict = np.load('./args_dict.npy', allow_pickle=True).item()
        args_dict.lr = 0.0005
        args_dict.decay_epochs = 15
        args_dict.warmup_epochs = 3
        args_dict.cooldown_epochs = 5
        args_dict.patience_epochs = 5
        
        aggregator = {"scores": [], "segmentations": []}
        basic_aggregator = {"scores": [], "segmentations": []}
        start_time = time.time()
        pr_auroc = 0
        basic_pr_auroc = 0
        
        optimizer = create_optimizer(args_dict, patchcore.prompt_model)
        epochs = self.args.epochs_num
        patchcore.prompt_model.train()
        patchcore.prompt_model.train_contrastive = True
        
        if args_dict.sched != 'constant':
            lr_scheduler, _ = create_scheduler(args_dict, optimizer)
        else:
            lr_scheduler = None
            
        best_metrics = {
            'auroc': 0, 'full_pixel_auroc': 0, 'img_ap': 0,
            'pixel_ap': 0, 'pixel_pro': 0, 'time_cost': 0
        }
        best_basic_metrics = {
            'auroc': 0, 'full_pixel_auroc': 0, 'img_ap': 0,
            'pixel_ap': 0, 'pixel_pro': 0, 'time_cost': 0
        }

        # ì—í¬í¬ë³„ í›ˆë ¨
        print(f"\n{'='*60}")
        print(f"Training Dataset: {dataset_name}")
        print(f"Total Epochs: {epochs}")
        print(f"{'='*60}")
        
        for epoch in range(epochs):
            epoch_start_time = time.time()
            torch.cuda.empty_cache()
            
            # í›ˆë ¨ ë‹¨ê³„
            patchcore.prompt_model.train()
            loss_list = []
                                    
            batch_count = 0
            total_batches = len(dataloaders["training"])
            
            for image in dataloaders["training"]:
                if isinstance(image, dict):
                    image_paths = image["image_path"]
                    image = image["image"].cuda()
                
                res = patchcore._embed_train_sam(image, provide_patch_shapes=True, image_path=image_paths)
                loss = res['loss']
                loss_list.append(loss.item())
                
                optimizer.zero_grad()
                if loss != 0:
                    loss.backward()
                torch.nn.utils.clip_grad_norm_(patchcore.prompt_model.parameters(), args_dict.clip_grad)
                optimizer.step()
                
                batch_count += 1
                # 10% ë‹¨ìœ„ë¡œ ì§„í–‰ë¥  í‘œì‹œ
                if batch_count % max(1, total_batches // 10) == 0:
                    progress = int(batch_count / total_batches * 100)
                    print(f"{progress}%", end=" ", flush=True)
            
            epoch_train_time = time.time() - epoch_start_time
            avg_loss = np.mean(loss_list)
            print(f"Done! (Loss: {avg_loss:.4f}, Time: {epoch_train_time:.1f}s)")
            
            if lr_scheduler:
                lr_scheduler.step(epoch)
            
            # í‰ê°€ ë‹¨ê³„
            eval_start_time = time.time()
            patchcore.prompt_model.eval()
            
            print(f"Epoch {epoch+1}/{epochs} - Evaluating...", end=" ", flush=True)
            
            # ë¬´ì œí•œ ë©”ëª¨ë¦¬ í‰ê°€
            nolimimit_memory_feature = patchcore.fit_with_limit_size_prompt(dataloaders["training"], self.args.basic_size)
            patchcore.anomaly_scorer.fit(detection_features=[nolimimit_memory_feature])
            basic_scores, basic_segmentations, basic_labels_gt, basic_masks_gt = patchcore.predict_prompt(
                dataloaders["testing"]
            )
            basic_aggregator["scores"].append(basic_scores)
            basic_aggregator["segmentations"].append(basic_segmentations)
            basic_end_time = time.time()
            
            # ì œí•œëœ ë©”ëª¨ë¦¬ í‰ê°€
            memory_feature = patchcore.fit_with_limit_size_prompt(dataloaders["training"], self.args.memory_size)
            patchcore.anomaly_scorer.fit(detection_features=[memory_feature])
            scores, segmentations, labels_gt, masks_gt = patchcore.predict_prompt(
                dataloaders["testing"]
            )
            aggregator["scores"].append(scores)
            aggregator["segmentations"].append(segmentations)
            end_time = time.time()
            
            eval_time = end_time - eval_start_time
            print(f"Done! (Time: {eval_time:.1f}s)")
            
            # ë©”íŠ¸ë¦­ ê³„ì‚° ë° ì—…ë°ì´íŠ¸
            current_metrics = self.compute_metrics(aggregator, masks_gt, basic_end_time, end_time, dataloaders)
            current_basic_metrics = self.compute_basic_metrics(basic_aggregator, basic_masks_gt, start_time, basic_end_time, dataloaders)
            
            # ì„±ëŠ¥ ë³€í™” í‘œì‹œ
            print(f"\nğŸ“Š Epoch {epoch+1} Results:")
            print(f"   Limited Memory   - AUROC: {current_metrics['auroc']:.4f}, Pixel AUROC: {current_metrics['full_pixel_auroc']:.4f}, PRO: {current_metrics['pixel_pro']:.4f}")
            print(f"   Unlimited Memory - AUROC: {current_basic_metrics['auroc']:.4f}, Pixel AUROC: {current_basic_metrics['full_pixel_auroc']:.4f}, PRO: {current_basic_metrics['pixel_pro']:.4f}")
            
            # ëª¨ë“  ì—í¬í¬ ê²°ê³¼ ì €ì¥ (CSVì— ëˆ„ì )
            self.result_manager.save_epoch_result(dataset_name, epoch + 1, 'limited_memory', current_metrics)
            self.result_manager.save_epoch_result(dataset_name, epoch + 1, 'unlimited_memory', current_basic_metrics)
            
            # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸ ë° ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            improved = False
            if current_metrics['auroc'] > pr_auroc:
                # ë©”ëª¨ë¦¬ì— ì €ì¥ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
                self.memory_feature_list[dataloader_count] = memory_feature
                self.prompt_list[dataloader_count] = patchcore.prompt_model.get_cur_prompt()
                
                pr_auroc = current_metrics['auroc']
                best_metrics = current_metrics.copy()
                improved = True
                
                # ë² ìŠ¤íŠ¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                checkpoint_paths = self.checkpoint_manager.save_best_checkpoint(
                    dataset_name, 
                    patchcore.prompt_model.get_cur_prompt(),
                    memory_feature,
                    key_feature,
                    current_metrics
                )
                
                self.exp_logger.info(f"  ğŸ¯ Best checkpoint saved for {dataset_name}")
                self.exp_logger.info(f"     Prompt: {checkpoint_paths['prompt_path']}")
                self.exp_logger.info(f"     Memory: {checkpoint_paths['memory_path']}")
                self.exp_logger.info(f"     Metrics: {checkpoint_paths['metrics_path']}")
                
                if current_metrics['auroc'] == 1:
                    print(f"ğŸ¯ Perfect AUROC achieved! Early stopping.")
                    break
            
            if current_basic_metrics['auroc'] > basic_pr_auroc:
                basic_pr_auroc = current_basic_metrics['auroc']
                best_basic_metrics = current_basic_metrics.copy()
            
            # ì„±ëŠ¥ ë¡œê·¸ ê¸°ë¡
            self.log_epoch_performance(epoch, epochs, dataset_name, current_metrics, current_basic_metrics, improved)
            
            if improved:
                print(f"ğŸš€ New best Limited Memory AUROC: {pr_auroc:.4f} (â†‘)")
            else:
                print(f"   Best Limited Memory AUROC remains: {pr_auroc:.4f}")
            
            print(f"   Total epoch time: {time.time() - epoch_start_time:.1f}s")
            print(f"{'-'*60}")
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥ ë° ë¡œê·¸
        print(f"\nğŸ† Final Results for {dataset_name}:")
        self.print_final_results(dataloader_count, best_metrics, best_basic_metrics)
        self.log_final_dataset_results(dataset_name, best_metrics, best_basic_metrics)

    def compute_metrics(self, aggregator, masks_gt, basic_end_time, end_time, dataloaders):
        """ë©”íŠ¸ë¦­ ê³„ì‚°"""
        scores = np.array(aggregator["scores"])
        min_scores = scores.min(axis=-1).reshape(-1, 1)
        max_scores = scores.max(axis=-1).reshape(-1, 1)
        scores = (scores - min_scores) / (max_scores - min_scores)
        scores = np.mean(scores, axis=0)
        
        segmentations = np.array(aggregator["segmentations"])
        min_scores = (
            segmentations.reshape(len(segmentations), -1)
            .min(axis=-1)
            .reshape(-1, 1, 1, 1)
        )
        max_scores = (
            segmentations.reshape(len(segmentations), -1)
            .max(axis=-1)
            .reshape(-1, 1, 1, 1)
        )
        segmentations = (segmentations - min_scores) / (max_scores - min_scores)
        segmentations = np.mean(segmentations, axis=0)
        
        time_cost = (end_time - basic_end_time) / len(dataloaders["testing"])
        anomaly_labels = [
            x[1] != "good" for x in dataloaders["testing"].dataset.data_to_iterate
        ]
        
        ap_seg = np.asarray(segmentations)
        ap_seg = ap_seg.flatten()
        
        auroc = patchcore.metrics.compute_imagewise_retrieval_metrics(
            scores, anomaly_labels
        )["auroc"]
        
        ap_mask = np.asarray(masks_gt)
        ap_mask = ap_mask.flatten().astype(np.int32)
        pixel_ap = average_precision_score(ap_mask, ap_seg)
        img_ap = average_precision_score(anomaly_labels, scores)
        
        # í”½ì…€ ë‹¨ìœ„ ë©”íŠ¸ë¦­ ê³„ì‚°
        segmentations_reshaped = ap_seg.reshape(-1, 224, 224)
        pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(
            segmentations_reshaped, masks_gt
        )
        full_pixel_auroc = pixel_scores["auroc"]
        
        # PRO ìŠ¤ì½”ì–´ ê³„ì‚°
        for i, mask in enumerate(masks_gt):
            masks_gt[i] = np.array(mask[0])
        for i, seg in enumerate(segmentations_reshaped):
            segmentations_reshaped[i] = np.array(seg)
        pixel_pro, _ = calculate_au_pro(np.array(masks_gt), np.array(segmentations_reshaped))
        
        return {
            'auroc': auroc,
            'full_pixel_auroc': full_pixel_auroc,
            'img_ap': img_ap,
            'pixel_ap': pixel_ap,
            'pixel_pro': pixel_pro,
            'time_cost': time_cost
        }

    def compute_basic_metrics(self, basic_aggregator, basic_masks_gt, start_time, basic_end_time, dataloaders):
        """ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚° (ë¬´ì œí•œ ë©”ëª¨ë¦¬)"""
        basic_scores = np.array(basic_aggregator["scores"])
        basic_min_scores = basic_scores.min(axis=-1).reshape(-1, 1)
        basic_max_scores = basic_scores.max(axis=-1).reshape(-1, 1)
        basic_scores = (basic_scores - basic_min_scores) / (basic_max_scores - basic_min_scores)
        basic_scores = np.mean(basic_scores, axis=0)
        
        basic_segmentations = np.array(basic_aggregator["segmentations"])
        basic_min_scores = (
            basic_segmentations.reshape(len(basic_segmentations), -1)
            .min(axis=-1)
            .reshape(-1, 1, 1, 1)
        )
        basic_max_scores = (
            basic_segmentations.reshape(len(basic_segmentations), -1)
            .max(axis=-1)
            .reshape(-1, 1, 1, 1)
        )
        basic_segmentations = (basic_segmentations - basic_min_scores) / (basic_max_scores - basic_min_scores)
        basic_segmentations = np.mean(basic_segmentations, axis=0)
        
        basic_time_cost = (basic_end_time - start_time) / len(dataloaders["testing"])
        basic_anomaly_labels = [
            x[1] != "good" for x in dataloaders["testing"].dataset.data_to_iterate
        ]
        
        basic_ap_seg = np.asarray(basic_segmentations)
        basic_ap_seg = basic_ap_seg.flatten()
        
        basic_auroc = patchcore.metrics.compute_imagewise_retrieval_metrics(
            basic_scores, basic_anomaly_labels
        )["auroc"]
        
        basic_ap_mask = np.asarray(basic_masks_gt)
        basic_ap_mask = basic_ap_mask.flatten().astype(np.int32)
        basic_pixel_ap = average_precision_score(basic_ap_mask, basic_ap_seg)
        basic_img_ap = average_precision_score(basic_anomaly_labels, basic_scores)
        
        # ê¸°ë³¸ í”½ì…€ ë‹¨ìœ„ ë©”íŠ¸ë¦­
        basic_segmentations_reshaped = basic_ap_seg.reshape(-1, 224, 224)
        basic_pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(
            basic_segmentations_reshaped, basic_masks_gt
        )
        basic_full_pixel_auroc = basic_pixel_scores["auroc"]
        
        # ê¸°ë³¸ PRO ìŠ¤ì½”ì–´
        for i, mask in enumerate(basic_masks_gt):
            basic_masks_gt[i] = np.array(mask[0])
        for i, seg in enumerate(basic_segmentations_reshaped):
            basic_segmentations_reshaped[i] = np.array(seg)
        basic_pixel_pro, _ = calculate_au_pro(np.array(basic_masks_gt), np.array(basic_segmentations_reshaped))
        
        return {
            'auroc': basic_auroc,
            'full_pixel_auroc': basic_full_pixel_auroc,
            'img_ap': basic_img_ap,
            'pixel_ap': basic_pixel_ap,
            'pixel_pro': basic_pixel_pro,
            'time_cost': basic_time_cost
        }

    def print_final_results(self, dataloader_count, best_metrics, best_basic_metrics):
        """ìµœì¢… ê²°ê³¼ ì¶œë ¥"""
        print(f"   ğŸ“Š Limited Memory Results:")
        print(f"      - Image AUROC: {best_metrics['auroc']:.4f}")
        print(f"      - Pixel AUROC: {best_metrics['full_pixel_auroc']:.4f}")
        print(f"      - Image AP: {best_metrics['img_ap']:.4f}")
        print(f"      - Pixel AP: {best_metrics['pixel_ap']:.4f}")
        print(f"      - Pixel PRO: {best_metrics['pixel_pro']:.4f}")
        print(f"      - Time Cost: {best_metrics['time_cost']:.4f}s")
        
        print(f"   ğŸ“Š Unlimited Memory Results:")
        print(f"      - Image AUROC: {best_basic_metrics['auroc']:.4f}")
        print(f"      - Pixel AUROC: {best_basic_metrics['full_pixel_auroc']:.4f}")
        print(f"      - Image AP: {best_basic_metrics['img_ap']:.4f}")
        print(f"      - Pixel AP: {best_basic_metrics['pixel_ap']:.4f}")
        print(f"      - Pixel PRO: {best_basic_metrics['pixel_pro']:.4f}")
        print(f"      - Time Cost: {best_basic_metrics['time_cost']:.4f}s")

    def log_dataset_start(self, dataset_name, dataloader_count, total_datasets):
        """ë°ì´í„°ì…‹ ì‹œì‘ ë¡œê·¸"""
        self.exp_logger.info("="*80)
        self.exp_logger.info(f"Dataset ({dataloader_count + 1}/{total_datasets}): {dataset_name}")
        self.exp_logger.info("="*80)

    def log_epoch_performance(self, epoch, total_epochs, dataset_name, current_metrics, current_basic_metrics, improved):
        """ì—í¬í¬ë³„ ì„±ëŠ¥ ë¡œê·¸"""
        self.exp_logger.info(f"Epoch {epoch+1}/{total_epochs} - {dataset_name}")
        self.exp_logger.info(f"  Limited Memory   - AUROC: {current_metrics['auroc']:.4f}, Pixel AUROC: {current_metrics['full_pixel_auroc']:.4f}, Image AP: {current_metrics['img_ap']:.4f}, Pixel AP: {current_metrics['pixel_ap']:.4f}, PRO: {current_metrics['pixel_pro']:.4f}")
        self.exp_logger.info(f"  Unlimited Memory - AUROC: {current_basic_metrics['auroc']:.4f}, Pixel AUROC: {current_basic_metrics['full_pixel_auroc']:.4f}, Image AP: {current_basic_metrics['img_ap']:.4f}, Pixel AP: {current_basic_metrics['pixel_ap']:.4f}, PRO: {current_basic_metrics['pixel_pro']:.4f}")
        if improved:
            self.exp_logger.info(f"  *** NEW BEST Limited Memory AUROC: {current_metrics['auroc']:.4f} ***")

    def log_final_dataset_results(self, dataset_name, best_metrics, best_basic_metrics):
        """ë°ì´í„°ì…‹ë³„ ìµœì¢… ê²°ê³¼ ë¡œê·¸"""
        self.exp_logger.info("-"*60)
        self.exp_logger.info(f"FINAL RESULTS - {dataset_name}")
        self.exp_logger.info("-"*60)
        self.exp_logger.info("Limited Memory Results:")
        self.exp_logger.info(f"  - Image AUROC: {best_metrics['auroc']:.4f}")
        self.exp_logger.info(f"  - Pixel AUROC: {best_metrics['full_pixel_auroc']:.4f}")
        self.exp_logger.info(f"  - Image AP: {best_metrics['img_ap']:.4f}")
        self.exp_logger.info(f"  - Pixel AP: {best_metrics['pixel_ap']:.4f}")
        self.exp_logger.info(f"  - Pixel PRO: {best_metrics['pixel_pro']:.4f}")
        self.exp_logger.info(f"  - Time Cost: {best_metrics['time_cost']:.4f}s")
        
        self.exp_logger.info("Unlimited Memory Results:")
        self.exp_logger.info(f"  - Image AUROC: {best_basic_metrics['auroc']:.4f}")
        self.exp_logger.info(f"  - Pixel AUROC: {best_basic_metrics['full_pixel_auroc']:.4f}")
        self.exp_logger.info(f"  - Image AP: {best_basic_metrics['img_ap']:.4f}")
        self.exp_logger.info(f"  - Pixel AP: {best_basic_metrics['pixel_ap']:.4f}")
        self.exp_logger.info(f"  - Pixel PRO: {best_basic_metrics['pixel_pro']:.4f}")
        self.exp_logger.info(f"  - Time Cost: {best_basic_metrics['time_cost']:.4f}s")
        self.exp_logger.info("")

    def log_overall_summary(self):
        """ì „ì²´ ê²°ê³¼ ìš”ì•½ ë¡œê·¸"""
        self.exp_logger.info("="*80)
        self.exp_logger.info("OVERALL PERFORMANCE SUMMARY")
        self.exp_logger.info("="*80)
        
        best_results = self.result_manager.get_best_results_summary()
        if best_results:
            self.exp_logger.info("Best Results Summary:")
            for result in best_results:
                dataset_name = result["dataset_name"]
                auroc = result["image_auroc"]
                pixel_auroc = result["pixel_auroc"]
                img_ap = result["image_ap"]
                pixel_pro = result["pixel_pro"]
                self.exp_logger.info(f"  {dataset_name:15s} - AUROC: {auroc:.4f}, Pixel AUROC: {pixel_auroc:.4f}, Image AP: {img_ap:.4f}, PRO: {pixel_pro:.4f}")
            
            avg_auroc = sum(result["image_auroc"] for result in best_results) / len(best_results)
            avg_pixel_auroc = sum(result["pixel_auroc"] for result in best_results) / len(best_results)
            avg_img_ap = sum(result["image_ap"] for result in best_results) / len(best_results)
            avg_pixel_pro = sum(result["pixel_pro"] for result in best_results) / len(best_results)
            self.exp_logger.info(f"  {'AVERAGE':15s} - AUROC: {avg_auroc:.4f}, Pixel AUROC: {avg_pixel_auroc:.4f}, Image AP: {avg_img_ap:.4f}, PRO: {avg_pixel_pro:.4f}")
            self.exp_logger.info("")
        
        self.exp_logger.info("="*80)

    def inference(self):
        """ëª¨ë“  continual learning ì™„ë£Œ í›„ ìµœì¢… inference ìˆ˜í–‰"""
        print(f"\n{'='*80}")
        print("ğŸ” Continual Learning Inference ì‹œì‘...")
        print(f"{'='*80}")
        
        # ì‹¤í—˜ ë¡œê±°ì— inference ì‹œì‘ ë¡œê·¸
        self.exp_logger.info("="*80)
        self.exp_logger.info("CONTINUAL LEARNING INFERENCE STARTED")
        self.exp_logger.info("="*80)
        
        # ë°ì´í„°ë¡œë” ì¬ìƒì„±
        list_of_dataloaders = self.create_dataloaders()
        inference_results = []
        
        # ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ inference ìˆ˜í–‰
        for dataloader_count, dataloaders in enumerate(list_of_dataloaders):
            dataset_name = dataloaders["training"].name
            
            print(f"\nğŸ” Inference Dataset ({dataloader_count + 1}/{len(list_of_dataloaders)}): {dataset_name}")
            
            LOGGER.info(
                "Inference on dataset [{}] ({}/{})...".format(
                    dataset_name,
                    dataloader_count + 1,
                    len(list_of_dataloaders),
                )
            )
            
            # ì‹œë“œ ê³ ì •
            patchcore.utils.fix_seeds(self.args.seed, self.device)
            
            with self.device_context:
                torch.cuda.empty_cache()
                
                # PatchCore ëª¨ë¸ ìƒì„±
                imagesize = dataloaders["training"].dataset.imagesize
                sampler = self.create_sampler()
                patchcore_list = self.create_patchcore_model(imagesize, sampler)
                
                if len(patchcore_list) > 1:
                    LOGGER.info(
                        "Utilizing PatchCore Ensemble (N={}).".format(len(patchcore_list))
                    )
                
                aggregator = {"scores": [], "segmentations": []}
                
                # ê° PatchCore ëª¨ë¸ì— ëŒ€í•´ inference
                for i, patchcore in enumerate(patchcore_list):
                    torch.cuda.empty_cache()
                    
                    LOGGER.info(
                        "Inference with model ({}/{})".format(i + 1, len(patchcore_list))
                    )
                    
                    start_time = time.time()
                    
                    # ì¿¼ë¦¬ ê¸°ë°˜ íƒœìŠ¤í¬ ì„ íƒ: ê° íƒœìŠ¤í¬ì˜ key featureë¡œ í˜„ì¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì™€ ìœ ì‚¬ë„ ê³„ì‚°
                    print("   ğŸ” íƒœìŠ¤í¬ ì„ íƒì„ ìœ„í•œ ì¿¼ë¦¬ ê³„ì‚° ì¤‘...", end=" ", flush=True)
                    cur_query_list = []
                    
                    for key_count in range(len(list_of_dataloaders)):
                        # ìœ íš¨í•œ key featureê°€ ìˆëŠ”ì§€ í™•ì¸
                        if self.key_feature_list[key_count] == 0:
                            cur_query_list.append(float('inf'))  # í›ˆë ¨ë˜ì§€ ì•Šì€ íƒœìŠ¤í¬ëŠ” ë¬´í•œëŒ€
                            continue
                            
                        # í•´ë‹¹ íƒœìŠ¤í¬ì˜ key featureë¡œ anomaly scorer ì„¤ì •
                        patchcore.anomaly_scorer.fit(detection_features=[self.key_feature_list[key_count]])
                        
                        # í˜„ì¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì´ìƒ ì ìˆ˜ ê³„ì‚°
                        query_scores, query_seg, labels_gt_query, masks_gt_query = patchcore.predict(
                            dataloaders["testing"]
                        )
                        
                        # ì´ ì´ìƒ ì ìˆ˜ í•©ê³„ (ë‚®ì„ìˆ˜ë¡ í•´ë‹¹ íƒœìŠ¤í¬ì™€ ìœ ì‚¬)
                        cur_query_list.append(np.sum(query_scores))
                    
                    print("Done!")
                    
                    # ê°€ì¥ ë‚®ì€ ì´ìƒ ì ìˆ˜ë¥¼ ê°€ì§„ íƒœìŠ¤í¬ ì„ íƒ
                    print(f"   ğŸ“Š Query scores: {[f'{score:.2f}' if score != float('inf') else 'N/A' for score in cur_query_list]}")
                    query_data_id = np.argmin(cur_query_list)
                    selected_task_name = self.args.subdatasets[query_data_id]
                    
                    print(f"   ğŸ¯ ì„ íƒëœ íƒœìŠ¤í¬: {selected_task_name} (ID: {query_data_id})")
                    
                    # ì„ íƒëœ íƒœìŠ¤í¬ì˜ ì„¤ì • ì ìš©
                    patchcore.set_dataloadercount(query_data_id)
                    
                    # ì„ íƒëœ íƒœìŠ¤í¬ì˜ í”„ë¡¬í”„íŠ¸ ì ìš©
                    if self.prompt_list[query_data_id] != 0:
                        patchcore.prompt_model.set_cur_prompt(self.prompt_list[query_data_id])
                    
                    # í‰ê°€ ëª¨ë“œ ì„¤ì •
                    patchcore.prompt_model.eval()
                    
                    # ì„ íƒëœ íƒœìŠ¤í¬ì˜ ë©”ëª¨ë¦¬ íŠ¹ì§•ìœ¼ë¡œ anomaly scorer ì„¤ì •
                    if self.memory_feature_list[query_data_id] != 0:
                        patchcore.anomaly_scorer.fit(detection_features=[self.memory_feature_list[query_data_id]])
                    
                    print("   ğŸ”® ìµœì¢… ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...", end=" ", flush=True)
                    
                    # í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ìµœì¢… ì˜ˆì¸¡
                    scores, segmentations, labels_gt, masks_gt = patchcore.predict_prompt(
                        dataloaders["testing"]
                    )
                    
                    aggregator["scores"].append(scores)
                    aggregator["segmentations"].append(segmentations)
                    
                    print("Done!")
                
                # ì•™ìƒë¸” ê²°ê³¼ ì²˜ë¦¬
                scores = np.array(aggregator["scores"])
                min_scores = scores.min(axis=-1).reshape(-1, 1)
                max_scores = scores.max(axis=-1).reshape(-1, 1)
                scores = (scores - min_scores) / (max_scores - min_scores)
                scores = np.mean(scores, axis=0)
                
                segmentations = np.array(aggregator["segmentations"])
                min_scores = (
                    segmentations.reshape(len(segmentations), -1)
                    .min(axis=-1)
                    .reshape(-1, 1, 1, 1)
                )
                max_scores = (
                    segmentations.reshape(len(segmentations), -1)
                    .max(axis=-1)
                    .reshape(-1, 1, 1, 1)
                )
                segmentations = (segmentations - min_scores) / (max_scores - min_scores)
                segmentations = np.mean(segmentations, axis=0)
                
                end_time = time.time()
                time_cost = (end_time - start_time) / len(dataloaders["testing"])
                
                # ì´ìƒ ë¼ë²¨ ì¶”ì¶œ
                anomaly_labels = [
                    x[1] != "good" for x in dataloaders["testing"].dataset.data_to_iterate
                ]
                
                # ë©”íŠ¸ë¦­ ê³„ì‚°
                print("   ğŸ“ˆ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...", end=" ", flush=True)
                
                ap_seg = np.asarray(segmentations)
                ap_seg = ap_seg.flatten()
                
                LOGGER.info("Computing evaluation metrics.")
                
                # Image AUROC
                auroc = patchcore.metrics.compute_imagewise_retrieval_metrics(
                    scores, anomaly_labels
                )["auroc"]
                
                # Pixel ë©”íŠ¸ë¦­
                ap_mask = np.asarray(masks_gt)
                ap_mask = ap_mask.flatten().astype(np.int32)
                pixel_ap = average_precision_score(ap_mask, ap_seg)
                img_ap = average_precision_score(anomaly_labels, scores)
                
                # Pixel AUROC
                segmentations_reshaped = ap_seg.reshape(-1, 224, 224)
                pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(
                    segmentations_reshaped, masks_gt
                )
                full_pixel_auroc = pixel_scores["auroc"]
                
                # PRO ìŠ¤ì½”ì–´
                for i, mask in enumerate(masks_gt):
                    masks_gt[i] = np.array(mask[0])
                for i, seg in enumerate(segmentations_reshaped):
                    segmentations_reshaped[i] = np.array(seg)
                pixel_pro, _ = calculate_au_pro(np.array(masks_gt), np.array(segmentations_reshaped))
                
                print("Done!")
                
                # ê²°ê³¼ ì €ì¥
                inference_metrics = {
                    "image_auroc": auroc,
                    "pixel_auroc": full_pixel_auroc,
                    "image_ap": img_ap,
                    "pixel_ap": pixel_ap,
                    "pixel_pro": pixel_pro,
                    "time_cost": time_cost
                }
                
                # Final.csvì— ì €ì¥
                self.result_manager.save_final_result(
                    dataset_name, 
                    selected_task_name, 
                    query_data_id, 
                    inference_metrics
                )
                
                # ê²°ê³¼ ì¶œë ¥
                print(f"\n   ğŸ“Š Inference Results for {dataset_name}:")
                print(f"      ğŸ¯ Selected Task: {selected_task_name}")
                print(f"      ğŸ“ˆ Image AUROC: {auroc:.4f}")
                print(f"      ğŸ“ˆ Pixel AUROC: {full_pixel_auroc:.4f}")
                print(f"      ğŸ“ˆ Image AP: {img_ap:.4f}")
                print(f"      ğŸ“ˆ Pixel AP: {pixel_ap:.4f}")
                print(f"      ğŸ“ˆ Pixel PRO: {pixel_pro:.4f}")
                print(f"      â±ï¸  Time Cost: {time_cost:.4f}s")
                
                # ì„±ëŠ¥ ë¡œê·¸ ê¸°ë¡
                self.exp_logger.info(f"Inference - {dataset_name}")
                self.exp_logger.info(f"  Selected Task: {selected_task_name} (ID: {query_data_id})")
                self.exp_logger.info(f"  Image AUROC: {auroc:.4f}")
                self.exp_logger.info(f"  Pixel AUROC: {full_pixel_auroc:.4f}")
                self.exp_logger.info(f"  Image AP: {img_ap:.4f}")
                self.exp_logger.info(f"  Pixel AP: {pixel_ap:.4f}")
                self.exp_logger.info(f"  Pixel PRO: {pixel_pro:.4f}")
                self.exp_logger.info(f"  Time Cost: {time_cost:.4f}s")
                self.exp_logger.info("")
                
                print(f'   ğŸ“‹ Test task: {dataloader_count+1}/{len(list_of_dataloaders)}, '
                      f'Selected task: {selected_task_name}, '
                      f'Image AUROC: {auroc:.4f}, '
                      f'Pixel AUROC: {full_pixel_auroc:.4f}, '
                      f'Image AP: {img_ap:.4f}, '
                      f'Pixel AP: {pixel_ap:.4f}, '
                      f'Pixel PRO: {pixel_pro:.4f}, '
                      f'Time cost: {time_cost:.4f}')
        
        # ìµœì¢… inference ê²°ê³¼ ìš”ì•½
        self.log_inference_summary()
        
        print(f"\n{'='*80}")
        print("âœ… Continual Learning Inference ì™„ë£Œ!")
        print(f"ğŸ“ Inference ê²°ê³¼ ì €ì¥ë¨: {self.result_manager.final_csv_path}")
        print(f"{'='*80}")
        
        return inference_results

    def log_inference_summary(self):
        """Inference ê²°ê³¼ ìš”ì•½ ë¡œê·¸"""
        self.exp_logger.info("="*80)
        self.exp_logger.info("CONTINUAL LEARNING INFERENCE SUMMARY")
        self.exp_logger.info("="*80)
        
        # Final.csvì—ì„œ ê²°ê³¼ ì½ì–´ì˜¤ê¸°
        if os.path.exists(self.result_manager.final_csv_path):
            df = pd.read_csv(self.result_manager.final_csv_path)
            if not df.empty:
                # íƒœìŠ¤í¬ ì„ íƒ í†µê³„
                task_selection_count = df['selected_task'].value_counts().to_dict()
                
                self.exp_logger.info("Task Selection Statistics:")
                for task, count in task_selection_count.items():
                    self.exp_logger.info(f"  {task}: {count} times")
                self.exp_logger.info("")
                
                self.exp_logger.info("Detailed Results:")
                for _, row in df.iterrows():
                    self.exp_logger.info(f"  {row['test_dataset']:15s} -> {row['selected_task']:15s} | "
                                       f"AUROC: {row['image_auroc']:.4f}, Pixel AUROC: {row['pixel_auroc']:.4f}, "
                                       f"Image AP: {row['image_ap']:.4f}, Pixel AP: {row['pixel_ap']:.4f}, "
                                       f"PRO: {row['pixel_pro']:.4f}, Time: {row['time_cost']:.4f}s")
                
                # í‰ê·  ê³„ì‚°
                avg_auroc = df['image_auroc'].mean()
                avg_pixel_auroc = df['pixel_auroc'].mean()
                avg_img_ap = df['image_ap'].mean()
                avg_pixel_ap = df['pixel_ap'].mean()
                avg_pixel_pro = df['pixel_pro'].mean()
                avg_time = df['time_cost'].mean()
                
                self.exp_logger.info("-"*80)
                self.exp_logger.info(f"  {'AVERAGE':15s} {'':18s} | "
                                   f"AUROC: {avg_auroc:.4f}, Pixel AUROC: {avg_pixel_auroc:.4f}, "
                                   f"Image AP: {avg_img_ap:.4f}, Pixel AP: {avg_pixel_ap:.4f}, "
                                   f"PRO: {avg_pixel_pro:.4f}, Time: {avg_time:.4f}s")
        
        self.exp_logger.info("="*80)

    def run(self):
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        print("ğŸš€ UCAD í›ˆë ¨ ì‹œì‘...")
        
        # ì‹¤í—˜ ë¡œê±° ì´ˆê¸°í™” ë¡œê·¸
        self.exp_logger.info("UCAD Training Started")
        self.exp_logger.info(f"Memory Size: {self.args.memory_size}")
        self.exp_logger.info(f"Epochs: {self.args.epochs_num}")
        self.exp_logger.info(f"Batch Size: {self.args.batch_size}")
        self.exp_logger.info(f"Subdatasets: {self.args.subdatasets}")
        self.exp_logger.info("")
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        list_of_dataloaders = self.create_dataloaders()
        total_datasets = len(list_of_dataloaders)
        print(f"ğŸ“‚ ì´ {total_datasets}ê°œ ë°ì´í„°ì…‹ ë¡œë“œë¨: {self.args.subdatasets}")
        
        print(f"\n{'='*80}")
        print(f"ğŸ¯ í›ˆë ¨ ì„¤ì •")
        print(f"   - ì‹¤í—˜ ë””ë ‰í† ë¦¬: {self.experiment_dir}")
        print(f"   - ë©”ëª¨ë¦¬ í¬ê¸°: {self.args.memory_size}")
        print(f"   - ì—í¬í¬ ìˆ˜: {self.args.epochs_num}")
        print(f"   - ë°°ì¹˜ í¬ê¸°: {self.args.batch_size}")
        print(f"   - ì„¤ì • íŒŒì¼: {self.config_path}")
        print(f"   - ë¡œê·¸ íŒŒì¼: {self.log_path}")
        print(f"   - ê²°ê³¼ CSV: {self.result_manager.result_csv_path}")
        print(f"   - ì²´í¬í¬ì¸íŠ¸: {self.checkpoint_manager.checkpoint_dir}")
        print(f"{'='*80}\n")
        
        # ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ í›ˆë ¨
        for dataloader_count, dataloaders in enumerate(list_of_dataloaders):
            dataset_progress = f"({dataloader_count + 1}/{total_datasets})"
            print(f"\nğŸ”¥ Dataset {dataset_progress}: {dataloaders['training'].name}")
            
            LOGGER.info(
                "Evaluating dataset [{}] ({}/{})...".format(
                    dataloaders["training"].name,
                    dataloader_count + 1,
                    len(list_of_dataloaders),
                )
            )

            patchcore.utils.fix_seeds(self.args.seed, self.device)
            
            with self.device_context:
                torch.cuda.empty_cache()
                imagesize = dataloaders["training"].dataset.imagesize
                sampler = self.create_sampler()
                patchcore_list = self.create_patchcore_model(imagesize, sampler)                
                
                if len(patchcore_list) > 1:
                    LOGGER.info(
                        "Utilizing PatchCore Ensemble (N={}).".format(len(patchcore_list))
                    )
                
                # ë‹¨ì¼ ë°ì´í„°ì…‹ í›ˆë ¨
                self.train_single_dataset(dataloader_count, dataloaders, patchcore_list)
                
            print(f"\nâœ… Dataset {dataset_progress} ì™„ë£Œ!")
            LOGGER.info("\n\n-----\n")
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        print(f"\n{'='*80}")
        print("ğŸ“Š ëª¨ë“  ë°ì´í„°ì…‹ í›ˆë ¨ ì™„ë£Œ! ìµœì¢… ê²°ê³¼ ì €ì¥ ì¤‘...")
        self.save_final_results()
        
        print(f"\nğŸ‰ ì‹¤í—˜ ì™„ë£Œ! ëª¨ë“  ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:")
        print(f"   ğŸ“ ì‹¤í—˜ ë””ë ‰í† ë¦¬: {self.experiment_dir}")
        print(f"   ğŸ“Š í›ˆë ¨ ê²°ê³¼: {self.result_manager.result_csv_path}")
        print(f"   ğŸ’¾ ì²´í¬í¬ì¸íŠ¸: {self.checkpoint_manager.checkpoint_dir}")
        print(f"   ğŸ“ ë¡œê·¸ íŒŒì¼: {self.log_path}")
        print(f"   âš™ï¸ ì„¤ì • íŒŒì¼: {self.config_path}")
        print(f"{'='*80}")

        # ì‹¤í—˜ ì™„ë£Œ ë¡œê·¸
        self.exp_logger.info("="*80)
        self.exp_logger.info("UCAD TRAINING COMPLETED SUCCESSFULLY")
        self.exp_logger.info(f"Experiment Directory: {self.experiment_dir}")
        self.exp_logger.info(f"Training Results: {self.result_manager.result_csv_path}")
        self.exp_logger.info(f"Checkpoints: {self.checkpoint_manager.checkpoint_dir}")
        self.exp_logger.info("="*80)

    def save_final_results(self):
        """ìµœì¢… ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
        print('\nğŸ“ˆ í›ˆë ¨ ê²°ê³¼ ìš”ì•½:')
        
        # result.csvì—ì„œ ìµœê³  ì„±ëŠ¥ ê²°ê³¼ ì½ì–´ì˜¤ê¸°
        if os.path.exists(self.result_manager.result_csv_path):
            df = pd.read_csv(self.result_manager.result_csv_path)
            if not df.empty:
                print("   ì—í¬í¬ë³„ ê²°ê³¼ê°€ result.csvì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                # ê° ë°ì´í„°ì…‹ë³„ ìµœê³  ì„±ëŠ¥ ì¶œë ¥
                datasets = df['dataset_name'].unique()
                
                print('\n   ğŸ“Š ê° ë°ì´í„°ì…‹ë³„ ìµœê³  ì„±ëŠ¥:')
                for dataset in datasets:
                    dataset_df = df[df['dataset_name'] == dataset]
                    
                    # Limited Memory ìµœê³  ì„±ëŠ¥
                    limited_df = dataset_df[dataset_df['split_type'] == 'limited_memory']
                    if not limited_df.empty:
                        best_limited = limited_df.loc[limited_df['image_auroc'].idxmax()]
                        print(f"      {dataset} (Limited)  : AUROC={best_limited['image_auroc']:.4f} (epoch {best_limited['epoch']})")
                    
                    # Unlimited Memory ìµœê³  ì„±ëŠ¥
                    unlimited_df = dataset_df[dataset_df['split_type'] == 'unlimited_memory']
                    if not unlimited_df.empty:
                        best_unlimited = unlimited_df.loc[unlimited_df['image_auroc'].idxmax()]
                        print(f"      {dataset} (Unlimited): AUROC={best_unlimited['image_auroc']:.4f} (epoch {best_unlimited['epoch']})")
                
                # ì „ì²´ í‰ê·  ê³„ì‚°
                limited_avg = df[df['split_type'] == 'limited_memory'].groupby('dataset_name')['image_auroc'].max().mean()
                unlimited_avg = df[df['split_type'] == 'unlimited_memory'].groupby('dataset_name')['image_auroc'].max().mean()
                
                print(f'\n   ğŸ“ˆ ì „ì²´ í‰ê·  ì„±ëŠ¥:')
                print(f"      Limited Memory Average AUROC  : {limited_avg:.4f}")
                print(f"      Unlimited Memory Average AUROC: {unlimited_avg:.4f}")
                
                # ì‹¤í—˜ ë¡œê±°ì—ë„ ê¸°ë¡
                self.exp_logger.info("Training Results Summary:")
                self.exp_logger.info(f"Limited Memory Average AUROC: {limited_avg:.4f}")
                self.exp_logger.info(f"Unlimited Memory Average AUROC: {unlimited_avg:.4f}")
                
                # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì¶œë ¥
                print(f'\n   ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ„ì¹˜: {self.checkpoint_manager.checkpoint_dir}')
                checkpoint_files = [f for f in os.listdir(self.checkpoint_manager.checkpoint_dir) if f.endswith('.pkl') or f.endswith('.json')]
                if checkpoint_files:
                    print(f"      ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ìˆ˜: {len(checkpoint_files)}")
                    self.exp_logger.info(f"Checkpoints saved: {len(checkpoint_files)} files in {self.checkpoint_manager.checkpoint_dir}")
        
        # ì „ì²´ ìš”ì•½ ì„±ëŠ¥ ë¡œê·¸ ê¸°ë¡
        self.log_overall_summary()


def parse_arguments():
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='UCAD í›ˆë ¨ ë° í‰ê°€')
    
    # ê¸°ë³¸ ì„¤ì •
    parser.add_argument('results_path', type=str, help='ê²°ê³¼ ì €ì¥ ê²½ë¡œ')
    parser.add_argument('--gpu', type=int, nargs='+', default=[0], help='ì‚¬ìš©í•  GPU ë²ˆí˜¸')
    parser.add_argument('--seed', type=int, default=0, help='ëœë¤ ì‹œë“œ')
    parser.add_argument('--log_group', type=str, default="group", help='ë¡œê·¸ ê·¸ë£¹ëª…')
    parser.add_argument('--log_project', type=str, default="project", help='í”„ë¡œì íŠ¸ëª…')
    parser.add_argument('--exp_name', type=str, default=None, help='ì‹¤í—˜ ì´ë¦„ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)')
    parser.add_argument('--save_segmentation_images', action='store_true', help='ì„¸ê·¸ë©˜í…Œì´ì…˜ ì´ë¯¸ì§€ ì €ì¥')
    parser.add_argument('--save_patchcore_model', action='store_true', help='PatchCore ëª¨ë¸ ì €ì¥')
    parser.add_argument('--memory_size', type=int, default=196, help='ë©”ëª¨ë¦¬ ë±…í¬ í¬ê¸°')
    parser.add_argument('--epochs_num', type=int, default=25, help='í›ˆë ¨ ì—í¬í¬ ìˆ˜')
    parser.add_argument('--key_size', type=int, default=196, help='í‚¤ íŠ¹ì§• í¬ê¸°')
    parser.add_argument('--basic_size', type=int, default=1960, help='ê¸°ë³¸ ë©”ëª¨ë¦¬ í¬ê¸°')
    
    # PatchCore ì„¤ì • (backbone ê´€ë ¨ ì œê±°)
    parser.add_argument('--pretrain_embed_dimension', type=int, default=1024, help='ì‚¬ì „í›ˆë ¨ ì„ë² ë”© ì°¨ì›')
    parser.add_argument('--target_embed_dimension', type=int, default=1024, help='íƒ€ê²Ÿ ì„ë² ë”© ì°¨ì›')
    parser.add_argument('--preprocessing', choices=["mean", "conv"], default="mean", help='ì „ì²˜ë¦¬ ë°©ë²•')
    parser.add_argument('--aggregation', choices=["mean", "mlp"], default="mean", help='ì§‘ê³„ ë°©ë²•')
    parser.add_argument('--anomaly_scorer_num_nn', type=int, default=5, help='ì´ìƒ ì ìˆ˜ ê³„ì‚°ìš© NN ê°œìˆ˜')
    parser.add_argument('--patchsize', type=int, default=3, help='íŒ¨ì¹˜ í¬ê¸°')
    parser.add_argument('--patchscore', type=str, default="max", help='íŒ¨ì¹˜ ì ìˆ˜ ê³„ì‚° ë°©ë²•')
    parser.add_argument('--patchoverlap', type=float, default=0.0, help='íŒ¨ì¹˜ ì˜¤ë²„ë©')
    parser.add_argument('--faiss_on_gpu', action='store_true', help='GPUì—ì„œ Faiss ì‹¤í–‰')
    parser.add_argument('--faiss_num_workers', type=int, default=8, help='Faiss ì›Œì»¤ ìˆ˜')
    
    # ìƒ˜í”ŒëŸ¬ ì„¤ì •
    parser.add_argument('--sampler_name', type=str, default="approx_greedy_coreset", help='ìƒ˜í”ŒëŸ¬ ì´ë¦„')
    parser.add_argument('--percentage', '-p', type=float, default=0.1, help='ìƒ˜í”Œë§ ë¹„ìœ¨')
    
    # ë°ì´í„°ì…‹ ì„¤ì •
    parser.add_argument('--dataset_name', type=str, default="mvtec", help='ë°ì´í„°ì…‹ ì´ë¦„')
    parser.add_argument('--data_path', type=str, default="/Volume/VAD/UCAD/mvtec2d", help='ë°ì´í„° ê²½ë¡œ')
    parser.add_argument('--subdatasets', '-d', type=str, nargs='+', 
                       default=['bottle', 'cable', 'capsule', 'carpet', 'grid', 'hazelnut', 'leather', 
                               'metal_nut', 'pill', 'screw', 'tile', 'toothbrush', 'transistor', 'wood', 'zipper'],
                       help='ì„œë¸Œ ë°ì´í„°ì…‹ ëª©ë¡')
    parser.add_argument('--train_val_split', type=float, default=1, help='í›ˆë ¨/ê²€ì¦ ë¶„í•  ë¹„ìœ¨')
    parser.add_argument('--batch_size', type=int, default=8, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--num_workers', type=int, default=8, help='ë°ì´í„°ë¡œë” ì›Œì»¤ ìˆ˜')
    parser.add_argument('--resize', type=int, default=224, help='ë¦¬ì‚¬ì´ì¦ˆ í¬ê¸°')
    parser.add_argument('--imagesize', type=int, default=224, help='ì´ë¯¸ì§€ í¬ê¸°')
    parser.add_argument('--augment', action='store_true', help='ë°ì´í„° ì¦ê°• ì‚¬ìš©')
    
    return parser.parse_args()


# PRO ê³„ì‚° ê´€ë ¨ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
class GroundTruthComponent:
    def __init__(self, anomaly_scores):
        self.anomaly_scores = anomaly_scores.copy()
        self.anomaly_scores.sort()
        self.index = 0
        self.last_threshold = None

    def compute_overlap(self, threshold):
        if self.last_threshold is not None:
            assert self.last_threshold <= threshold

        while (self.index < len(self.anomaly_scores) and self.anomaly_scores[self.index] <= threshold):
            self.index += 1

        return 1.0 - self.index / len(self.anomaly_scores)


def trapezoid(x, y, x_max=None):
    x = np.array(x)
    y = np.array(y)
    finite_mask = np.logical_and(np.isfinite(x), np.isfinite(y))
    if not finite_mask.all():
        print("WARNING: Not all x and y values passed to trapezoid are finite. Will continue with only the finite values.")
    x = x[finite_mask]
    y = y[finite_mask]

    correction = 0.
    if x_max is not None:
        if x_max not in x:
            ins = bisect(x, x_max)
            assert 0 < ins < len(x)
            y_interp = y[ins - 1] + ((y[ins] - y[ins - 1]) * (x_max - x[ins - 1]) / (x[ins] - x[ins - 1]))
            correction = 0.5 * (y_interp + y[ins - 1]) * (x_max - x[ins - 1])

        mask = x <= x_max
        x = x[mask]
        y = y[mask]

    return np.sum(0.5 * (y[1:] + y[:-1]) * (x[1:] - x[:-1])) + correction


def collect_anomaly_scores(anomaly_maps, ground_truth_maps):
    assert len(anomaly_maps) == len(ground_truth_maps)

    ground_truth_components = []
    anomaly_scores_ok_pixels = np.zeros(len(ground_truth_maps) * ground_truth_maps[0].size)

    structure = np.ones((3, 3), dtype=int)

    ok_index = 0
    for gt_map, prediction in zip(ground_truth_maps, anomaly_maps):
        labeled, n_components = label(gt_map, structure)

        num_ok_pixels = len(prediction[labeled == 0])
        anomaly_scores_ok_pixels[ok_index:ok_index + num_ok_pixels] = prediction[labeled == 0].copy()
        ok_index += num_ok_pixels

        for k in range(n_components):
            component_scores = prediction[labeled == (k + 1)]
            ground_truth_components.append(GroundTruthComponent(component_scores))

    anomaly_scores_ok_pixels = np.resize(anomaly_scores_ok_pixels, ok_index)
    anomaly_scores_ok_pixels.sort()

    return ground_truth_components, anomaly_scores_ok_pixels


def compute_pro(anomaly_maps, ground_truth_maps, num_thresholds):
    ground_truth_components, anomaly_scores_ok_pixels = collect_anomaly_scores(anomaly_maps, ground_truth_maps)

    threshold_positions = np.linspace(0, len(anomaly_scores_ok_pixels) - 1, num=num_thresholds, dtype=int)

    fprs = [1.0]
    pros = [1.0]
    for pos in threshold_positions:
        threshold = anomaly_scores_ok_pixels[pos]

        fpr = 1.0 - (pos + 1) / len(anomaly_scores_ok_pixels)

        pro = 0.0
        for component in ground_truth_components:
            pro += component.compute_overlap(threshold)
        pro /= len(ground_truth_components)

        fprs.append(fpr)
        pros.append(pro)

    fprs = fprs[::-1]
    pros = pros[::-1]

    return fprs, pros


def calculate_au_pro(gts, predictions, integration_limit=0.3, num_thresholds=100):
    pro_curve = compute_pro(anomaly_maps=predictions, ground_truth_maps=gts, num_thresholds=num_thresholds)

    au_pro = trapezoid(pro_curve[0], pro_curve[1], x_max=integration_limit)
    au_pro /= integration_limit

    return au_pro, pro_curve


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    LOGGER.info("Command line arguments: {}".format(" ".join(sys.argv)))
    
    # ì¸ì íŒŒì‹±
    args = parse_arguments()
    
    # UCAD í›ˆë ¨ê¸° ìƒì„± ë° ì‹¤í–‰
    trainer = UCADTrainer(args)
    trainer.run()
    
    # Continual Learning ì™„ë£Œ í›„ ìµœì¢… Inference ìˆ˜í–‰
    trainer.inference()


if __name__ == "__main__":
    main() 